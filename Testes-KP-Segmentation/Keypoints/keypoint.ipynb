{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Package 'tensorflow-docs' requires a different Python: 3.8.10 not in '>=3.9'\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q imageio\n",
    "%pip install -q opencv-python\n",
    "%pip install -q matplotlib\n",
    "%pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 17:47:15.797547: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-02 17:47:15.798935: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 17:47:15.829935: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-02 17:47:15.830394: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-02 17:47:16.466755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_docs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embed\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "from tensorflow_docs.vis import embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions for visualization\n",
    "\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def label_keypoints(image_path,static_keypoints):\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    labeled_keypoints = {}\n",
    "    for keypoint, index in KEYPOINT_DICT.items():\n",
    "        y, x, conf = static_keypoints[0][0][index]\n",
    "        labeled_keypoints[keypoint] = {\n",
    "            'y': y,\n",
    "            'x': x,\n",
    "            'confidence': conf\n",
    "        }\n",
    "    \n",
    "    return labeled_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "input_size = 192\n",
    "\n",
    "def movenet(input_image):\n",
    "  model = module.signatures['serving_default']\n",
    "\n",
    "  # SavedModel format expects tensor type of int32.\n",
    "  input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "  # Run model inference.\n",
    "  outputs = model(input_image)\n",
    "  # Output is a [1, 1, 17, 3] tensor.\n",
    "  keypoints_with_scores = outputs['output_0'].numpy()\n",
    "  return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o input_image.jpeg https://images.pexels.com/photos/4384679/pexels-photo-4384679.jpeg --silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image.\n",
    "image_path = 'input_image.jpeg'\n",
    "image = tf.io.read_file(image_path)\n",
    "image = tf.image.decode_jpeg(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "input_image = tf.expand_dims(image, axis=0)\n",
    "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "# Run model inference.\n",
    "keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "keypointLeft = 'left_hip'\n",
    "keypointRight = 'right_hip'\n",
    "\n",
    "labeled_keypoints = label_keypoints(image_path, keypoints_with_scores)\n",
    "confidence_percent_left = labeled_keypoints[keypointLeft][\"confidence\"] * 100\n",
    "confidence_percent_right = labeled_keypoints[keypointRight][\"confidence\"] * 100\n",
    "\n",
    "print(f'Informações sobre {keypointLeft}:')\n",
    "print(f'y={labeled_keypoints[keypointRight][\"y\"]:.2f}, x={labeled_keypoints[keypointLeft][\"x\"]:.2f}, confidence={confidence_percent_left:.2f}%')\n",
    "\n",
    "print(f'\\nInformações sobre {keypointRight}:')\n",
    "print(f'y={labeled_keypoints[keypointRight][\"y\"]:.2f}, x={labeled_keypoints[keypointRight][\"x\"]:.2f}, confidence={confidence_percent_right:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
